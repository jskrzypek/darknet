[net]
# how many images are in each batch to average the loss over?
batch=64
# into how many sub-batches shall each batch be divided to handle images in each sub-batch in parallel? 
subdivisions=64
# input size of the network
height=448
width=448
channels=3
# learning parameters
momentum=0.9
decay=0.0005

# base learning rate
learning_rate=0.001
# change learning rate after the corresponding steps
policy=steps
# need to have as many steps as scale
steps=200,400,600,5000,15000
# re-scale the current learning rate by the correponding factor once the number of steps is reached
scales=2.5,2,2,.1,.1
# max number of "iterations"
max_batches=20000
# snapshow the learned weights after every k "iterations"
i_snapshot_iteration=2500
#
#
c_ending_gt_files=.txt

[crop]
crop_width=448
crop_height=448
flip=0
angle=0
saturation = 1.5
exposure = 1.5

[convolutional]
filters=64
size=7
stride=2
pad=1
activation=leaky

[maxpool]
size=2
stride=2

[convolutional]
filters=192
size=3
stride=1
pad=1
activation=leaky

[maxpool]
size=2
stride=2

[convolutional]
filters=128
size=1
stride=1
pad=1
activation=leaky

[convolutional]
filters=256
size=3
stride=1
pad=1
activation=leaky

[convolutional]
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
filters=512
size=3
stride=1
pad=1
activation=leaky

[maxpool]
size=2
stride=2

[convolutional]
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
filters=512
size=3
stride=1
pad=1
activation=leaky

[convolutional]
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
filters=512
size=3
stride=1
pad=1
activation=leaky

[convolutional]
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
filters=512
size=3
stride=1
pad=1
activation=leaky

[convolutional]
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
filters=512
size=3
stride=1
pad=1
activation=leaky

[convolutional]
filters=512
size=1
stride=1
pad=1
activation=leaky

[convolutional]
filters=1024
size=3
stride=1
pad=1
activation=leaky

[maxpool]
size=2
stride=2

[convolutional]
filters=512
size=1
stride=1
pad=1
activation=leaky

[convolutional]
filters=1024
size=3
stride=1
pad=1
activation=leaky

[convolutional]
filters=512
size=1
stride=1
pad=1
activation=leaky

[convolutional]
filters=1024
size=3
stride=1
pad=1
activation=leaky

#######

[convolutional]
size=3
stride=1
pad=1
filters=1024
activation=leaky

[convolutional]
size=3
stride=2
pad=1
filters=1024
activation=leaky

[convolutional]
size=3
stride=1
pad=1
filters=1024
activation=leaky

[convolutional]
size=3
stride=1
pad=1
filters=1024
activation=leaky

[connected]
output=4096
activation=leaky

[dropout]
probability=.5

[connected]
#needs to be side*side*((1 + l.coords)*l.n + l.classes) as defined in the detection layer below
# e.g., 20 classes, side 7 -> 1470
# e.g., 20 class, side 9 -> 2430
output= 1470
activation=linear

[detection]
# number of categories to compute class probabilites
classes=1
# number of parameters of a detection hypotheses
coords=4
rescore=1
b_debug=0
# number of cells per dimension
side=9
# number of boxes each cell predicts
num=2
softmax=0
sqrt=1
jitter=.2

object_scale=1
noobject_scale=.5
class_scale=1
coord_scale=5

